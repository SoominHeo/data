We're the new technologists.

We have a lot of data, so we have a lot of power.

How much power do we have?

Scene from a movie: "Apocalypse Now" -- great movie.

The way we're going to do this is fly him in and drop him off.

So the scene: the sky is filled with this fleet of helicopters carrying him in.

That's the kind of power I feel in this room.

That's the kind of power we have because of all of the data that we have. 

Let's take an example.

I can look at your financial records.

I can tell if you pay your bills on time.

I know if you're good to give a loan to.

When you come to my website, I actually know what you're going to do already because I've seen you visit millions of websites before.

And I'm sorry to tell you, you're like a poker player, you have a tell.

I can tell with data analysis what you're going to do before you even do it.

Those are the kinds of things we can do with the data that we have.

But I'm not actually here to talk about what we can do.

I'm here to talk about what we should do.

What's the right thing to do? 

But it brings me back.

We gather together these physicists in Los Alamos to see what they'll build.

We want the people building the technology thinking about what we should be doing with the technology. 

So what should we be doing with that guy's data?

Or should we respect his privacy, protect his dignity and leave him alone?

Which one is it?

How should we figure it out? 

Or should we leave him alone?

Collect his data.

(Laughter) Okay, last question -- harder question -- when trying to evaluate what we should do in this case, should we use a Kantian deontological moral framework, or should we use a Millian consequentialist one?

How do we know what to do with all the power we have if we don't have a moral framework?

We know more about mobile operating systems, but what we really need is a moral operating system.

What's a moral operating system?

We all know right and wrong, right?

You feel good when you do something right, you feel bad when you do something wrong.

Our parents teach us that: praise with the good, scold with the bad.

But how do we figure out what's right and wrong?

And from day to day, we have the techniques that we use.

Maybe we just follow our gut.

Maybe we take a vote -- we crowdsource.

And maybe, if we want to be on surer footing, what we really want is a moral framework that will help guide us there, that will tell us what kinds of things are right and wrong in the first place, and how would we know in a given situation what to do. 

So let's get a moral framework.

We're numbers people, living by numbers.

How can we use numbers as the basis for a moral framework?

I know a guy who did exactly that.

A brilliant guy -- he's been dead 2,500 years.

Plato, that's right.

And Plato, he had a lot of the same concerns that we did.

He was worried about right and wrong.

He wanted to know what is just.

It's kind of convincing when he talks and when she talks too.

I'm just going back and forth; I'm not getting anywhere.

I don't want opinions; I want knowledge.

Take a number, any number -- two.

There are truths about two.

If you've got two of something, you add two more, you get four.

That's true no matter what thing you're talking about.

They all participate in the truths that two has.

They all have two-ness in them.

And therefore, it's not a matter of opinion. 

What if, Plato thought, ethics was like math?

What if there were a pure form of justice?

What if there are truths about justice, and you could just look around in this world and see which things participated, partook of that form of justice?

Then you would know what was really just and what wasn't.

It wouldn't be a matter of just opinion or just appearances.

That's as ambitious as we are.

He wants to solve ethics.

He wants objective truths.

If you think that way, you have a Platonist moral framework. 

If you don't think that way, well, you have a lot of company in the history of Western philosophy, because the tidy idea, you know, people criticized it.

Aristotle, in particular, he was not amused.

He thought it was impractical.

He thought ethics was a matter of making decisions in the here-and-now using our best judgment to find the right path.

If you think that, Plato's not your guy.

But don't give up.

How about this: What if in any situation you could just calculate, look at the choices, measure out which one's better and know what to do?

So basis of utilitarianism -- I'm sure you're familiar at least.

The three people who voted for Mill before are familiar with this.

But here's the way it works.

What if morals, what if what makes something moral is just a matter of if it maximizes pleasure and minimizes pain?

It does something intrinsic to the act.

It's not like its relation to some abstract form.

It's just a matter of the consequences.

You just look at the consequences and see if, overall, it's for the good or for the worse.

Let's take an example.

Suppose I go up and I say, "I'm going to take your phone."

And what if he's been sending little messages to Bin Laden's hideout -- or whoever took over after Bin Laden -- and he's actually like a terrorist, a sleeper cell.

I'm going to find that out, and when I find that out, I'm going to prevent a huge amount of damage that he could cause.

That has a very high utility to prevent that damage.

If you feel that way, that's a utilitarian choice. 

But maybe you don't feel that way either.

So let's stop.

We're right in the thick of it, this philosophical thicket.

And this goes on for thousands of years, because these are hard questions, and I've only got 15 minutes.

So let's cut to the chase.

How should we be making our decisions?

What's the formula that we can use in any situation to determine what we should do, whether we should use that guy's data or not?

What's the formula?

There's not a formula.

There's not a simple answer. 

Ethics is hard.

Ethics requires thinking.

And that's uncomfortable.

I know; I spent a lot of my career in artificial intelligence, trying to build machines that could do some of this thinking for us, that could give us answers.

But they can't.

You can't just take human thinking and put it into a machine.

We're the ones who have to do it.

Happily, we're not machines, and we can do it.

Not only can we think, we must.

And the response to that is that we demand the exercise of thinking from every sane person. 

In fact, let's start right now.

It is so important we are going to spend 30 seconds of valuable TEDTalk time doing nothing but thinking about this.

Now the next step -- try this.

Do it over lunch.

And don't just find another technologist friend; find somebody different than you.

Find an artist or a writer -- or, heaven forbid, find a philosopher and talk to them.

Just a few days ago, right across the street from here, there was hundreds of people gathered together.

It was technologists and humanists at that big BiblioTech Conference.

And they gathered together because the technologists wanted to learn what it would be like to think from a humanities perspective.

You have someone from Google talking to someone who does comparative literature.

We're moved because it's about the battle between good and evil, about right and wrong.

And we care about right and wrong.

We care what happens in that opera.

We care what happens in "Apocalypse Now."

And we certainly care what happens with our technologies. 

We're the ones writing this opera.

This is our movie.

We figure out what will happen with this technology.

We determine how this will all end. 

Thank you. 

