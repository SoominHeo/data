So I want to show you a clip of the film now.

Hopefully it won't stutter.

And if we did our jobs well, you won't know that we were even involved. 

Voice (Video): I don't know how it's possible ... but you seem to have more hair. 

I was born with some form of disease. 

Voice: What kind of disease? 

BP: I was born old. 

Girl: Are you sick? 

Girl: You're different than anybody I've ever met. 

BB: There were many changes ... some you could see, some you couldn't.

Ed Ulbrich: That was a clip from "The Curious Case of Benjamin Button."

Now, there's no use of prosthetic makeup or photography of Brad superimposed over another actor's body.

We've created a completely digital human head. 

So I'd like to start with a little bit of history on the project.

This is based on an F. Scott Fitzgerald short story.

It's about a man who's born old and lives his life in reverse.

Now, this movie has floated around Hollywood for well over half a century, and we first got involved with the project in the early '90s, with Ron Howard as the director.

We took a lot of meetings and we seriously considered it.

But at the time we had to throw in the towel.

It was deemed impossible.

It was beyond the technology of the day to depict a man aging backwards.

The human form, in particular the human head, has been considered the Holy Grail of our industry. 

The project came back to us about a decade later, and this time with a director named David Fincher.

Now, Fincher is an interesting guy.

David is fearless of technology, and he is absolutely tenacious.

And David won't take "no."

And David believed, like we do in the visual effects industry, that anything is possible as long as you have enough time, resources and, of course, money.

And so David had an interesting take on the film, and he threw a challenge at us.

We even ruled out the idea of using makeup.

And David wanted to carve deeply into Brad's face to bring the aging to this character.

He needed to be a very sympathetic character.

Sounded great. 

But we believed that we had a very solid methodology that might work ... 

But despite our verbal assurances, they wanted some proof.

And so, in 2004, they commissioned us to do a screen test of Benjamin.

And we did it in about five weeks.

But we used lots of cheats and shortcuts.

After many years of starts and stops on this project, and making that tough decision, they finally decided to greenlight the movie.

(Laughter) You know, this is some tough stuff. 

We had to hold up an hour of a movie with a character.

And it's not a special effects film; it has to be a man.

But we did know one thing.

Being from the visual effects industry, we, with David, believed that we now had enough time, enough resources, and, God, we hoped we had enough money.

And we had enough passion to will the processes and technology into existence. 

So, when you're faced with something like that, of course you've got to break it down.

So we had three main areas that we had to focus on.

And we also needed to create a character that could hold up under, really, all conditions.

He needed to be able to walk in broad daylight, at nighttime, under candlelight, he had to hold an extreme close-up, he had to deliver dialogue, he had to be able to run, he had to be able to sweat, he had to be able to take a bath, to cry, he even had to throw up.

Not all at the same time -- but he had to, you know, do all of those things. 

And the work had to hold up for almost the first hour of the movie.

We did about 325 shots.

So we needed a system that would allow Benjamin to do everything a human being can do.

And we realized that there was a giant chasm between the state of the art of technology in 2004 and where we needed it to be. 

So we focused on motion capture.

I'm sure many of you have seen motion capture.

The state of the art at the time was something called marker-based motion capture.

I'll give you an example here.

And then animators can take the data of the motion of those markers and apply them to a computer-generated character.

You can see the computer characters on the right are having the same complex motion as the dancers. 

And as you can see, it gives you a pretty crappy performance.

That's not terribly compelling.

And what we realized was that what we needed was the information that was going on between the markers.

We needed the subtleties of the skin.

We needed to see skin moving over muscle moving over bone.

We needed creases and dimples and wrinkles and all of those things. 

Our first revelation was to completely abort and walk away from the technology of the day, the status quo, the state of the art.

So we were left with this idea that we ended up calling "technology stew."

And we had to create kind of a sauce.

And the sauce was code in software that we'd written to allow these disparate pieces of technology to come together and work as one. 

Initially, we came across some remarkable research done by a gentleman named Dr. Paul Ekman in the early '70s.

He believed that he could, in fact, catalog the human face.

And he came up with this idea of Facial Action Coding System, or FACS.

He believed that there were 70 basic poses or shapes of the human face, and that those basic poses or shapes of the face can be combined to create infinite possibilities of everything the human face is capable of doing.

And of course, these transcend age, race, culture, gender.

So this became the foundation of our research as we went forward. 

And then we came across some remarkable technology called Contour.

And here you can see a subject having phosphorus makeup stippled on her face.

And now what we're looking at is really creating a surface capture as opposed to a marker capture.

The subject stands in front of a computer array of cameras, and those cameras can, frame-by-frame, reconstruct the geometry of exactly what the subject's doing at the moment.

So, effectively, you get 3D data in real time of the subject.

And if you look in a comparison, on the left, we see what volumetric data gives us and on the right you see what markers give us.

So, clearly, we were in a substantially better place for this.

But these were the early days of this technology, and it wasn't really proven yet.

We measure complexity and fidelity of data in terms of polygonal count.

And so, on the left, we were seeing 100,000 polygons.

This was when we had our "Aha!"

This was the breakthrough.

(Laughter) 

From there, we actually carved up those faces into smaller pieces and components of his face.

We need to put another 40 years on him at this point.

We brought in Rick Baker, and Rick is one of the great makeup and special effects gurus of our industry.

And we commissioned them to make a maquette, or a bust, of Benjamin.

So, in the spirit of "The Great Unveiling" -- I had to do this -- I had to unveil something.

So this is Ben 80.

Now, this was made from a life cast of Brad.

So, in fact, anatomically, it is correct.

The eyes, the jaw, the teeth: everything is in perfect alignment with what the real guy has.

And so now we had three age increments of Benjamin in the computer. 

But we needed to get a database of him doing more than that.

We went through this process, then, called retargeting.

This is Brad doing one of the Ekman FACS poses.

And here's the resulting data that comes from that, the model that comes from that.

Next we had to go into the shooting process.

So while all that's going on, we're down in New Orleans and locations around the world.

And we shot our body actors, and we shot them wearing blue hoods.

But now we needed to get Brad's performance to drive our virtual Benjamin.

We shot him with four HD cameras so we'd get multiple views of him and then David would choose the take of Brad being Benjamin that he thought best matched the footage with the rest of the cast. 

From there we went into a process called image analysis.

And so here, you can see again, the chosen take.

And you are seeing, now, that data being transposed on to Ben 87.

And so, what's interesting about this is we used something called image analysis, which is taking timings from different components of Benjamin's face.

And then, the sauce I talked about with our technology stew -- that secret sauce was, effectively, software that allowed us to match the performance footage of Brad in live action with our database of aged Benjamin, the FACS shapes that we had.

On a frame-by-frame basis, we could actually reconstruct a 3D head that exactly matched the performance of Brad. 

And then here's the reconstructed performance now with the timings of the performance.

And then, again, the final shot.

We had to create a lighting system.

So really, a big part of our processes was creating a lighting environment for every single location that Benjamin had to appear so that we could put Ben's head into any scene and it would exactly match the lighting that's on the other actors in the real world. 

We also had to create an eye system.

We found the old adage, you know, "The eyes are the window to the soul," absolutely true.

So the key here was to keep everybody looking in Ben's eyes.

And if you could feel the warmth, and feel the humanity, and feel his intent coming through the eyes, then we would succeed.

So we had one person focused on the eye system for almost two full years. 

We also had to create an articulating tongue that allowed him to enunciate his words.

There was a whole system written in software to articulate the tongue.

We had one person devoted to the tongue for about nine months.

He was very popular. 

Skin displacement: another big deal.

The skin had to be absolutely accurate.

This is a very, very, very early test in our process.

So, effectively we created a digital puppet that Brad Pitt could operate with his own face.

There were no animators necessary to come in and interpret behavior or enhance his performance. 

There was something that we encountered, though, that we ended up calling "the digital Botox effect."

So, as things went through this process, Fincher would always say, "It sandblasts the edges off of the performance."

And thing our process and the technology couldn't do, is they couldn't understand intent, the intent of the actor.

So it did take humans to kind of push it one way or another. 

