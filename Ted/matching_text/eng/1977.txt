But unlike the other speakers, I'm not going to tell you about the mysteries of the universe, or the wonders of evolution, or the really clever, innovative ways people are attacking the major inequalities in our world.

Or even the challenges of nation-states in the modern global economy.

My brief, as you've just heard, is to tell you about statistics -- and, to be more precise, to tell you some exciting things about statistics.

This evening, there's a reception in the University's Museum of Natural History.

It's very unlikely -- in this special setting, and this collection of people -- but you might just find yourself talking to someone you'd rather wish that you weren't.

(Laughter) Well, except they've been pre-warned now, and they'll know you're making it up.

And then one of two things will happen.

And you'll be left in peace to talk to the person you really want to talk to. 

It's one of the challenges in our profession to try and explain what we do.

We're not top on people's lists for dinner party guests and conversations and so on.

I was coming back to visit her.

She told this to one of her colleagues, who said, "Well, what does your boyfriend do?"

(Laughter) Don't tell her I said that.

(Laughter) "He models genes."

(Laughter) "He models genes."

That is my first love, and that's what I'll tell you a little bit about.

So here's the scene for the first question I'm going to ask you.

Can you imagine tossing a coin successively?

And for some reason -- which shall remain rather vague -- we're interested in a particular pattern.

Here's one -- a head, followed by a tail, followed by a tail. 

So suppose we toss a coin repeatedly.

Then the pattern, head-tail-tail, that we've suddenly become fixated with happens here.

And you can count: one, two, three, four, five, six, seven, eight, nine, 10 -- it happens after the 10th toss.

So you might think there are more interesting things to do, but humor me for the moment.

The first time they do it, maybe it happens after the 10th toss, as here.

The second time, maybe it's after the fourth toss.

That's what I want this side to think about. 

So, on this side, you get out your coins, and you toss and toss and toss.

On this side, you've got a number -- the average number of tosses until head-tail-head. 

Either they're the same, or this one's bigger than this one, or this one's bigger than that one.

So what's going on here?

So what you want to do is compare the average number of tosses until we first see head-tail-head with the average number of tosses until we first see head-tail-tail. 

(Laughter) OK.

It takes longer, on average.

How could that be?

If you went head-tail-head-tail-head, you can cunningly get two occurrences of the pattern in only five tosses.

You can't do that with head-tail-tail.

That turns out to be important. 

There are two ways of thinking about this.

I'll give you one of them.

So imagine -- let's suppose we're doing it.

And you keep tossing, to wait for the next head, to get excited. 

On this side, there's a different experience.

It's the same for the first two parts of the sequence.

If it's a tail, you crack open the champagne.

So if you want to put a million things down amongst eight million positions and you can have some of them overlapping, the clumps will be further apart.

It's another way of getting the intuition. 

What's the point I want to make?

It's a very, very simple example, an easily stated question in probability, which every -- you're in good company -- everybody gets wrong.

This is my little diversion into my real passion, which is genetics.

When you toss a coin, you get a sequence of heads and tails.

How long will those chunks be?"

That's a rather trivial connection between probability and genetics.

And we'll hear some talks later in the conference specifically about that.

You know about the Human Genome Project.

That was a project which aimed to read one copy of the human genome.

The natural thing to do after you've done that -- and that's what this project, the International HapMap Project, which is a collaboration between labs in five or six different countries.

Why do we care about that?

Well, there are lots of reasons.

That's one big project.

To try and understand what it is about genetic differences that causes the diseases.

Why do we want to do that?

Because we understand very little about most human diseases.

We don't know what causes them.

So that's, as I said, the little diversion on my main love. 

Back to some of the more mundane issues of thinking about uncertainty.

It gets it right 99 percent of the time.

And I take one of you, or I take someone off the street, and I test them for the disease in question.

Let's suppose there's a test for HIV -- the virus that causes AIDS -- and the test says the person has the disease.

What's the chance that they do?

The test gets it right 99 percent of the time.

So a natural answer is 99 percent.

Who likes that answer?

Come on -- everyone's got to get involved.

Don't think you don't trust me anymore.

That's what you might think.

It's not the answer, and it's not because it's only part of the story.

It actually depends on how common or how rare the disease is.

So let me try and illustrate that.

Here's a little caricature of a million individuals.

And in fact, if this is the prevalence of the disease, about 100 will have the disease and the rest won't.

So now suppose we test them all.

What happens?

But there are so many of them that there'll be an enormous number of false positives.

Put that another way -- of all of them who test positive -- so here they are, the individuals involved -- less than one in 100 actually have the disease.

So even though we think the test is accurate, the important part of the story is there's another bit of information we need. 

Here's the key intuition.

What we have to do, once we know the test is positive, is to weigh up the plausibility, or the likelihood, of two competing explanations.

Each of those explanations has a likely bit and an unlikely bit.

The other explanation is that the person does have the disease -- that's unlikely -- but the test gets it right, which is likely.

And the number we end up with -- that number which is a little bit less than one in 100 -- is to do with how likely one of those explanations is relative to the other.

Each of them taken together is unlikely. 

Here's a more topical example of exactly the same thing.

For various reasons, she was later charged with murder.

And at the trial, her trial, a very distinguished pediatrician gave evidence that the chance of two cot deaths, innocent deaths, in a family like hers -- which was professional and non-smoking -- was one in 73 million.

To be put through the stress of the trial, convicted of murdering them -- and to spend time in a women's prison, where all the other prisoners think you killed your children -- is a really awful thing to happen to someone.

And it happened in large part here because the expert got the statistics horribly wrong, in two different ways. 

So where did he get the one in 73 million number?

He looked at some research, which said the chance of one cot death in a family like Sally Clark's is about one in 8,500.

So he said, "I'll assume that if you have one cot death in a family, the chance of a second child dying from cot death aren't changed."

So that's what statisticians would call an assumption of independence.

It's like saying, "If you toss a coin and get a head the first time, that won't affect the chance of getting a head the second time."

So if you toss a coin twice, the chance of getting a head twice are a half -- that's the chance the first time -- times a half -- the chance a second time.

Unfortunately here -- and, really, regrettably -- first of all, in a situation like this you'd have to verify it empirically.

And secondly, it's palpably false.

There are lots and lots of things that we don't know about sudden infant deaths.

So if a family suffers from one cot death, you'd put them in a high-risk group.

They've probably got these environmental risk factors and/or genetic risk factors we don't know about.

Nonetheless, that's how it was presented, and at trial nobody even argued it.

That's the first problem.

The second problem is, what does the number of one in 73 million mean?

It's exactly the same logical error as the logical error of thinking that after the disease test, which is 99 percent accurate, the chance of having the disease is 99 percent.

And the other one was the chance, a priori, that the person had the disease or not.

It's exactly the same in this context.

There are two things involved -- two parts to the explanation.

We want to know how likely, or relatively how likely, two different explanations are.

And the second part of the explanation is that she suffered an incredibly unlikely event.

Not as unlikely as one in 73 million, but nonetheless rather unlikely.

The other explanation is that she was guilty.

Now, we probably think a priori that's unlikely.

And then if she were trying to kill the children, she succeeded.

So the chance that she's innocent isn't one in 73 million.

We don't know what it is.

It has to do with weighing up the strength of the other evidence against her and the statistical evidence.

We know the children died.

What matters is how likely or unlikely, relative to each other, the two explanations are.

And they're both implausible.

There's a situation where errors in statistics had really profound and really unfortunate consequences.

In fact, there are two other women who were convicted on the basis of the evidence of this pediatrician, who have subsequently been released on appeal.

Many cases were reviewed.

And it's particularly topical because he's currently facing a disrepute charge at Britain's General Medical Council. 

So just to conclude -- what are the take-home messages from this?

Well, we know that randomness and uncertainty and chance are very much a part of our everyday life.

It's also true -- and, although, you, as a collective, are very special in many ways, you're completely typical in not getting the examples I gave right.

It's very well documented that people get things wrong.

They make errors of logic in reasoning with uncertainty.

We can cope with the subtleties of language brilliantly -- and there are interesting evolutionary questions about how we got here.

We are not good at reasoning with uncertainty.

That's an issue in our everyday lives.

All of quality control, which has had a major impact on industrial processing, is underpinned by statistics.

It's something we're bad at doing.

At the very least, we should recognize that, and we tend not to.

To go back to the legal context, at the Sally Clark trial all of the lawyers just accepted what the expert said.

On the other hand, he came out and effectively said, or implied, "I know how to reason with uncertainty.

So we need to understand where our competence is and isn't.

Exactly the same kinds of issues arose in the early days of DNA profiling, when scientists, and lawyers and in some cases judges, routinely misrepresented evidence.

Usually -- one hopes -- innocently, but misrepresented evidence.

Forensic scientists said, "The chance that this guy's innocent is one in three million."

Even if you believe the number, just like the 73 million to one, that's not what it meant.

And there have been celebrated appeal cases in Britain and elsewhere because of that. 

We sort of expect it of politicians and don't hope for much more.

Thanks very much. 

