Our society is more technologically advanced than ever.

We send people to the moon, we make phones that talk to us or customize radio stations that can play only music we like.

Yet, our most advanced machines and computers still struggle at this task.

We have made fabulous megapixel cameras, but we have not delivered sight to the blind.

Drones can fly over massive land, but don't have enough vision technology to help us to track the changes of the rainforests.

Security cameras are everywhere, but they do not alert us when a child is drowning in a swimming pool.

Photos and videos are becoming an integral part of global life.

Yet our most advanced software is still struggling at understanding and managing this enormous content.

Cameras can take pictures like this one by converting lights into a two-dimensional array of numbers known as pixels, but these are just lifeless numbers.

They do not carry meaning in themselves.

So for 15 years now, starting from my Ph.D. at Caltech and then leading Stanford's Vision Lab, I've been working with my mentors, collaborators and students to teach computers to see.

Our research field is called computer vision and machine learning.

It's part of the general field of artificial intelligence.

After all, a cat is just a collection of shapes and colors, and this is what we did in the early days of object modeling.

But what about this cat?

(Laughter) It's all curled up.

Now you have to add another shape and viewpoint to the object model.

But what if cats are hidden?

What about these silly cats?

Now you get my point.

So about eight years ago, a very simple and profound observation changed my thinking.

They learn this through real-world experiences and examples.

Luckily, we didn't have to mount a camera on our head and wait for many years.

We downloaded nearly a billion images and used crowdsourcing technology like the Amazon Mechanical Turk platform to help us to label these images.

In hindsight, this idea of using big data to train computer algorithms may seem obvious now, but back in 2007, it was not so obvious.

We were fairly alone on this journey for quite a while.

Some very friendly colleagues advised me to do something more useful for my tenure, and we were constantly struggling for research funding.

Once, I even joked to my graduate students that I would just reopen my dry cleaner's shop to fund ImageNet.

So we carried on.

In 2009, the ImageNet project delivered a database of 15 million images across 22,000 classes of objects and things organized by everyday English words.

In both quantity and quality, this was an unprecedented scale.

As an example, in the case of cats, we have more than 62,000 cats of all kinds of looks and poses and across all species of domestic and wild cats.

Just like the brain consists of billions of highly connected neurons, a basic operating unit in a neural network is a neuron-like node.

It takes input from other nodes and sends output to others.

In a typical neural network we use to train our object recognition model, it has 24 million nodes, 140 million parameters, and 15 billion connections.

That's an enormous model.

Powered by the massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model, the convolutional neural network blossomed in a way that no one expected.

It became the winning architecture to generate exciting new results in object recognition.

This is a computer telling us this picture contains a cat and where the cat is.

But surprisingly, car prices also correlate well with crime rates in cities, or voting patterns by zip codes. 

Has the computer already matched or even surpassed human capabilities?

Not so fast.

So far, we have just taught the computer to see objects.

This is like a small child learning to utter a few nouns.

It's an incredible accomplishment, but it's only the first step.

Soon, another developmental milestone will be hit, and children begin to communicate in sentences.

So to teach a computer to see a picture and generate sentences, the marriage between big data and machine learning algorithm has to take another step.

Now, the computer has to learn from both pictures as well as natural language sentences generated by humans.

So it has been a long journey.

To get from age zero to three was hard.

The real challenge is to go from three to 13 and far beyond.

Let me remind you with this picture of the boy and the cake again.

What the computer doesn't see is that this is a special Italian cake that's only served during Easter time.

This is my son Leo.

On my quest for visual intelligence, I think of Leo constantly and the future world he will live in.

When machines can see, doctors and nurses will have extra pairs of tireless eyes to help them to diagnose and take care of patients.

Cars will run smarter and safer on the road.

Robots, not just humans, will help us to brave the disaster zones to save the trapped and wounded.

Little by little, we're giving sight to the machines.

First, we teach them to see.

Thank you. 

