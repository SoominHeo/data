But we're not seeing all the waves out there.

In fact, what we see is less than a 10 trillionth of what's out there.

Now, it's not that these things are inherently unseeable.

Instead, our brains are sampling just a little bit of the world. 

Let's do a consciousness-raiser on this.

Imagine that you are a bloodhound dog.

Your whole world is about smelling.

Everything is about smell for you.

So one day, you stop in your tracks with a revelation.

(Laughter) 

But the question is, do we have to be stuck there?

So the way this works is, you take a microphone and you digitize the signal, and you put an electrode strip directly into the inner ear.

Or, with the retinal implant, you take a camera and you digitize the signal, and then you plug an electrode grid directly into the optic nerve.

Now, how do we understand that?

Your brain is locked in a vault of silence and darkness inside your skull.

Whatever information comes in, it just figures out what to do with it.

And this is a very efficient kind of machine.

The brain figures out what to do with the data that comes in.

And when you look across the animal kingdom, you find lots of peripheral devices.

So what this means is that nature doesn't have to continually redesign the brain.

Instead, with the principles of brain operation established, all nature has to worry about is designing new peripherals. 

It's just what we have inherited from a complex road of evolution.

Now, that might sound speculative, but the first paper demonstrating this was published in the journal Nature in 1969.

Now, there have been many modern incarnations of this.

It sounds like a cacophony, but after several weeks, blind people start getting pretty good at understanding what's in front of them just based on what they're hearing.

Your brain doesn't know where the signals come from.

It just figures out what to do with them. 

And here is what we wanted to do: we wanted to make it so that sound from the world gets converted in some way so that a deaf person can understand what is being said.

So here's the concept.

So as I'm speaking, the sound is getting translated to a pattern of vibration on the vest.

So as I'm speaking -- (Applause) -- the sound is getting translated into dynamic patterns of vibration.

I'm feeling the sonic world around me. 

David Eagleman: So Scott says a word, Jonathan feels it on the vest, and he writes it on the board. 

DE: Jonathan is able to translate this complicated pattern of vibrations into an understanding of what's being said. 

Now, we've been very encouraged by our results with sensory substitution, but what we've been thinking a lot about is sensory addition.

So here's an experiment we're doing in the lab.

A subject is feeling a real-time streaming feed from the Net of data for five seconds.

Then, two buttons appear, and he has to make a choice.

He doesn't know what's going on.

He makes a choice, and he gets feedback after one second.

Now, here's the thing: The subject has no idea what all the patterns mean, but we're seeing if he gets better at figuring out which button to press.

He doesn't know that what we're feeding is real-time data from the stock market, and he's making buy and sell decisions.

(Laughter) And the feedback is telling him whether he did the right thing or not.

(Laughter) 

(Laughter) (Applause) It's a bigger experience than a human can normally have. 

We're also expanding the umvelt of pilots.

It's essentially like he's extending his skin up there, far away. 

And that's just the beginning.

What we're envisioning is taking a modern cockpit full of gauges and instead of trying to read the whole thing, you feel it.

We live in a world of information now, and there is a difference between accessing big data and experiencing it. 

So I think there's really no end to the possibilities on the horizon for human expansion.

So the key is this: As we move into the future, we're going to increasingly be able to choose our own peripheral devices.

So the question now is, how do you want to go out and experience your universe? 

Thank you. 

(Laughter) 

DE: Well, that's right, I wouldn't have to write to NIH anymore. 

CA: Well look, just to be skeptical for a minute, I mean, this is amazing, but isn't most of the evidence so far that sensory substitution works, not necessarily that sensory addition works?

And when we look around at things like braille, for example, people are getting information through bumps on their fingers.

So I don't think we have any reason to think there's a theoretical limit that we know the edge of. 

CA: If this checks out, you're going to be deluged.

DE: I mean, I think there's a lot of applications here.

The key is this: Our visual systems are good at detecting blobs and edges, but they're really bad at what our world has become, which is screens with lots and lots of data.

We have to crawl that with our attentional systems.

So I think heavy machinery, safety, feeling the state of a factory, of your equipment, that's one place it'll go right away. 

