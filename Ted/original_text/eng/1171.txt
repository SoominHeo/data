id="t-0"&&Power.
id="t-2000"&&That is the word that comes to mind.
id="t-4000"&&We're the new technologists.
id="t-6000"&&We have a lot of data, so we have a lot of power.
id="t-9000"&&How much power do we have?
id="t-11000"&&Scene from a movie: "Apocalypse Now" -- great movie.
id="t-14000"&&We've got to get our hero, Captain Willard, to the mouth of the Nung River so he can go pursue Colonel Kurtz.
id="t-19000"&&The way we're going to do this is fly him in and drop him off.
id="t-21000"&&So the scene: the sky is filled with this fleet of helicopters carrying him in.
id="t-26000"&&And there's this loud, thrilling music in the background, this wild music.
id="t-30000"&&♫ Dum da ta da dum ♫ ♫ Dum da ta da dum ♫ ♫ Da ta da da ♫ That's a lot of power.
id="t-39000"&&That's the kind of power I feel in this room.
id="t-41000"&&That's the kind of power we have because of all of the data that we have. 

id="t-45000"&&Let's take an example.
id="t-47000"&&What can we do with just one person's data?
id="t-47000"&&What can we do with that guy's data?
id="t-56000"&&I can look at your financial records.
id="t-58000"&&I can tell if you pay your bills on time.
id="t-60000"&&I know if you're good to give a loan to.
id="t-62000"&&I can look at your medical records; I can see if your pump is still pumping -- see if you're good to offer insurance to.
id="t-68000"&&I can look at your clicking patterns.
id="t-70000"&&When you come to my website, I actually know what you're going to do already because I've seen you visit millions of websites before.
id="t-75000"&&And I'm sorry to tell you, you're like a poker player, you have a tell.
id="t-79000"&&I can tell with data analysis what you're going to do before you even do it.
id="t-79000"&&I can tell with data analysis what you're going to do before you even do it.
id="t-79000"&&I can tell with data analysis what you're going to do before you even do it.

id="t-90000"&&Those are the kinds of things we can do with the data that we have.
id="t-95000"&&But I'm not actually here to talk about what we can do.
id="t-101000"&&I'm here to talk about what we should do.
id="t-105000"&&What's the right thing to do? 

id="t-109000"&&Now I see some puzzled looks like, "Why are you asking us what's the right thing to do?
id="t-109000"&&Now I see some puzzled looks like, "Why are you asking us what's the right thing to do?
id="t-109000"&&Now I see some puzzled looks like, "Why are you asking us what's the right thing to do?
id="t-117000"&&Fair enough.
id="t-120000"&&But it brings me back.
id="t-122000"&&I think about World War II -- some of our great technologists then, some of our great physicists, studying nuclear fission and fusion -- just nuclear stuff.
id="t-132000"&&We gather together these physicists in Los Alamos to see what they'll build.
id="t-138000"&&We want the people building the technology thinking about what we should be doing with the technology. 

id="t-146000"&&So what should we be doing with that guy's data?
id="t-149000"&&Should we be collecting it, gathering it, so we can make his online experience better?
id="t-154000"&&So we can make money?
id="t-156000"&&So we can protect ourselves if he was up to no good?
id="t-160000"&&Or should we respect his privacy, protect his dignity and leave him alone?
id="t-167000"&&Which one is it?
id="t-170000"&&How should we figure it out? 

I know: crowdsource.
I know: crowdsource.
id="t-176000"&&So to get people warmed up, let's start with an easy question -- something I'm sure everybody here has an opinion about: iPhone versus Android.
id="t-186000"&&Let's do a show of hands -- iPhone.
id="t-189000"&&Uh huh.
id="t-191000"&&Android.
id="t-194000"&&You'd think with a bunch of smart people we wouldn't be such suckers just for the pretty phones.
id="t-198000"&&(Laughter) Next question, a little bit harder.
id="t-204000"&&Should we be collecting all of that guy's data to make his experiences better and to protect ourselves in case he's up to no good?
id="t-211000"&&Or should we leave him alone?
id="t-213000"&&Collect his data.
id="t-218000"&&Leave him alone.
id="t-218000"&&Leave him alone.
id="t-218000"&&Leave him alone.
id="t-198000"&&(Laughter) Okay, last question -- harder question -- when trying to evaluate what we should do in this case, should we use a Kantian deontological moral framework, or should we use a Millian consequentialist one?
id="t-244000"&&Kant.
id="t-247000"&&Mill.
id="t-250000"&&Not as many votes.
id="t-198000"&&(Laughter) Yeah, that's a terrifying result.
id="t-259000"&&Terrifying, because we have stronger opinions about our hand-held devices than about the moral framework we should use to guide our decisions. 

id="t-269000"&&How do we know what to do with all the power we have if we don't have a moral framework?
id="t-275000"&&We know more about mobile operating systems, but what we really need is a moral operating system.
id="t-283000"&&What's a moral operating system?
id="t-285000"&&We all know right and wrong, right?
id="t-287000"&&You feel good when you do something right, you feel bad when you do something wrong.
id="t-291000"&&Our parents teach us that: praise with the good, scold with the bad.
id="t-294000"&&But how do we figure out what's right and wrong?
id="t-297000"&&And from day to day, we have the techniques that we use.
id="t-300000"&&Maybe we just follow our gut.
id="t-303000"&&Maybe we take a vote -- we crowdsource.
id="t-306000"&&Or maybe we punt -- ask the legal department, see what they say.
id="t-311000"&&In other words, it's kind of random, kind of ad hoc, how we figure out what we should do.
id="t-318000"&&And maybe, if we want to be on surer footing, what we really want is a moral framework that will help guide us there, that will tell us what kinds of things are right and wrong in the first place, and how would we know in a given situation what to do. 

id="t-331000"&&So let's get a moral framework.
id="t-333000"&&We're numbers people, living by numbers.
id="t-336000"&&How can we use numbers as the basis for a moral framework?
id="t-341000"&&I know a guy who did exactly that.
id="t-344000"&&A brilliant guy -- he's been dead 2,500 years.
id="t-350000"&&Plato, that's right.
id="t-352000"&&Remember him -- old philosopher?
id="t-354000"&&You were sleeping during that class.
id="t-357000"&&And Plato, he had a lot of the same concerns that we did.
id="t-359000"&&He was worried about right and wrong.
id="t-361000"&&He wanted to know what is just.
id="t-363000"&&But he was worried that all we seem to be doing is trading opinions about this.
id="t-363000"&&But he was worried that all we seem to be doing is trading opinions about this.
id="t-363000"&&But he was worried that all we seem to be doing is trading opinions about this.
id="t-370000"&&It's kind of convincing when he talks and when she talks too.
id="t-372000"&&I'm just going back and forth; I'm not getting anywhere.
id="t-374000"&&I don't want opinions; I want knowledge.
id="t-377000"&&I want to know the truth about justice -- like we have truths in math.
id="t-383000"&&In math, we know the objective facts.
id="t-386000"&&Take a number, any number -- two.
id="t-386000"&&Take a number, any number -- two.
id="t-386000"&&Take a number, any number -- two.
id="t-390000"&&There are truths about two.
id="t-392000"&&If you've got two of something, you add two more, you get four.
id="t-396000"&&That's true no matter what thing you're talking about.
id="t-398000"&&It's an objective truth about the form of two, the abstract form.
id="t-402000"&&When you have two of anything -- two eyes, two ears, two noses, just two protrusions -- those all partake of the form of two.
id="t-409000"&&They all participate in the truths that two has.
id="t-413000"&&They all have two-ness in them.
id="t-415000"&&And therefore, it's not a matter of opinion. 

id="t-418000"&&What if, Plato thought, ethics was like math?
id="t-422000"&&What if there were a pure form of justice?
id="t-425000"&&What if there are truths about justice, and you could just look around in this world and see which things participated, partook of that form of justice?
id="t-434000"&&Then you would know what was really just and what wasn't.
id="t-437000"&&It wouldn't be a matter of just opinion or just appearances.
id="t-442000"&&That's a stunning vision.
id="t-442000"&&That's a stunning vision.
id="t-442000"&&That's a stunning vision.
id="t-442000"&&That's a stunning vision.
id="t-447000"&&That's as ambitious as we are.
id="t-449000"&&He wants to solve ethics.
id="t-451000"&&He wants objective truths.
id="t-453000"&&If you think that way, you have a Platonist moral framework. 

id="t-459000"&&If you don't think that way, well, you have a lot of company in the history of Western philosophy, because the tidy idea, you know, people criticized it.
id="t-466000"&&Aristotle, in particular, he was not amused.
id="t-469000"&&He thought it was impractical.
id="t-472000"&&Aristotle said, "We should seek only so much precision in each subject as that subject allows."
id="t-478000"&&Aristotle thought ethics wasn't a lot like math.
id="t-481000"&&He thought ethics was a matter of making decisions in the here-and-now using our best judgment to find the right path.
id="t-488000"&&If you think that, Plato's not your guy.
id="t-490000"&&But don't give up.
id="t-492000"&&Maybe there's another way that we can use numbers as the basis of our moral framework. 

id="t-498000"&&How about this: What if in any situation you could just calculate, look at the choices, measure out which one's better and know what to do?
id="t-508000"&&That sound familiar?
id="t-510000"&&That's a utilitarian moral framework.
id="t-513000"&&John Stuart Mill was a great advocate of this -- nice guy besides -- and only been dead 200 years.
id="t-519000"&&So basis of utilitarianism -- I'm sure you're familiar at least.
id="t-523000"&&The three people who voted for Mill before are familiar with this.
id="t-525000"&&But here's the way it works.
id="t-527000"&&What if morals, what if what makes something moral is just a matter of if it maximizes pleasure and minimizes pain?
id="t-534000"&&It does something intrinsic to the act.
id="t-537000"&&It's not like its relation to some abstract form.
id="t-539000"&&It's just a matter of the consequences.
id="t-541000"&&You just look at the consequences and see if, overall, it's for the good or for the worse.
id="t-541000"&&You just look at the consequences and see if, overall, it's for the good or for the worse.
id="t-541000"&&You just look at the consequences and see if, overall, it's for the good or for the worse.

id="t-45000"&&Let's take an example.
id="t-549000"&&Suppose I go up and I say, "I'm going to take your phone."
id="t-553000"&&Not just because it rang earlier, but I'm going to take it because I made a little calculation.
id="t-558000"&&I thought, that guy looks suspicious.
id="t-561000"&&And what if he's been sending little messages to Bin Laden's hideout -- or whoever took over after Bin Laden -- and he's actually like a terrorist, a sleeper cell.
id="t-569000"&&I'm going to find that out, and when I find that out, I'm going to prevent a huge amount of damage that he could cause.
id="t-575000"&&That has a very high utility to prevent that damage.
id="t-578000"&&And compared to the little pain that it's going to cause -- because it's going to be embarrassing when I'm looking on his phone and seeing that he has a Farmville problem and that whole bit -- that's overwhelmed by the value of looking at the phone.
id="t-590000"&&If you feel that way, that's a utilitarian choice. 

id="t-595000"&&But maybe you don't feel that way either.
id="t-598000"&&Maybe you think, it's his phone.
id="t-600000"&&It's wrong to take his phone because he's a person and he has rights and he has dignity, and we can't just interfere with that.
id="t-608000"&&He has autonomy.
id="t-610000"&&It doesn't matter what the calculations are.
id="t-612000"&&There are things that are intrinsically wrong -- like lying is wrong, like torturing innocent children is wrong.
id="t-620000"&&Kant was very good on this point, and he said it a little better than I'll say it.
id="t-625000"&&He said we should use our reason to figure out the rules by which we should guide our conduct, and then it is our duty to follow those rules.
id="t-633000"&&It's not a matter of calculation. 

id="t-636000"&&So let's stop.
id="t-638000"&&We're right in the thick of it, this philosophical thicket.
id="t-641000"&&And this goes on for thousands of years, because these are hard questions, and I've only got 15 minutes.
id="t-648000"&&So let's cut to the chase.
id="t-650000"&&How should we be making our decisions?
id="t-654000"&&Is it Plato, is it Aristotle, is it Kant, is it Mill?
id="t-654000"&&Is it Plato, is it Aristotle, is it Kant, is it Mill?
id="t-654000"&&Is it Plato, is it Aristotle, is it Kant, is it Mill?
id="t-659000"&&What's the formula that we can use in any situation to determine what we should do, whether we should use that guy's data or not?
id="t-666000"&&What's the formula?
id="t-670000"&&There's not a formula.
id="t-674000"&&There's not a simple answer. 

id="t-676000"&&Ethics is hard.
id="t-679000"&&Ethics requires thinking.
id="t-683000"&&And that's uncomfortable.
id="t-685000"&&I know; I spent a lot of my career in artificial intelligence, trying to build machines that could do some of this thinking for us, that could give us answers.
id="t-694000"&&But they can't.
id="t-696000"&&You can't just take human thinking and put it into a machine.
id="t-700000"&&We're the ones who have to do it.
id="t-703000"&&Happily, we're not machines, and we can do it.
id="t-706000"&&Not only can we think, we must.
id="t-710000"&&Hannah Arendt said, "The sad truth is that most evil done in this world is not done by people who choose to be evil.
id="t-720000"&&It arises from not thinking."
id="t-723000"&&That's what she called the "banality of evil."
id="t-727000"&&And the response to that is that we demand the exercise of thinking from every sane person. 

So let's do that.
So let's do that.
id="t-736000"&&In fact, let's start right now.
id="t-739000"&&Every person in this room do this: think of the last time you had a decision to make where you were worried to do the right thing, where you wondered, "What should I be doing?"
id="t-749000"&&Bring that to mind, and now reflect on that and say, "How did I come up that decision?
id="t-749000"&&Bring that to mind, and now reflect on that and say, "How did I come up that decision?
id="t-749000"&&Bring that to mind, and now reflect on that and say, "How did I come up that decision?
id="t-749000"&&Bring that to mind, and now reflect on that and say, "How did I come up that decision?
id="t-749000"&&Bring that to mind, and now reflect on that and say, "How did I come up that decision?
id="t-761000"&&Or now we have a few more choices.
id="t-764000"&&"Did I evaluate what would be the highest pleasure like Mill would?
id="t-768000"&&Or like Kant, did I use reason to figure out what was intrinsically right?"
id="t-768000"&&Or like Kant, did I use reason to figure out what was intrinsically right?"
id="t-768000"&&Or like Kant, did I use reason to figure out what was intrinsically right?"
id="t-768000"&&Or like Kant, did I use reason to figure out what was intrinsically right?"
id="t-774000"&&It is so important we are going to spend 30 seconds of valuable TEDTalk time doing nothing but thinking about this.
id="t-774000"&&It is so important we are going to spend 30 seconds of valuable TEDTalk time doing nothing but thinking about this.
id="t-774000"&&It is so important we are going to spend 30 seconds of valuable TEDTalk time doing nothing but thinking about this.

Stop.
Stop.
id="t-801000"&&What you just did, that's the first step towards taking responsibility for what we should do with all of our power. 

id="t-810000"&&Now the next step -- try this.
id="t-814000"&&Go find a friend and explain to them how you made that decision.
id="t-814000"&&Go find a friend and explain to them how you made that decision.
id="t-814000"&&Go find a friend and explain to them how you made that decision.
id="t-820000"&&Do it over lunch.
id="t-822000"&&And don't just find another technologist friend; find somebody different than you.
id="t-827000"&&Find an artist or a writer -- or, heaven forbid, find a philosopher and talk to them.
id="t-832000"&&In fact, find somebody from the humanities.
id="t-832000"&&In fact, find somebody from the humanities.
id="t-832000"&&In fact, find somebody from the humanities.
id="t-838000"&&Just a few days ago, right across the street from here, there was hundreds of people gathered together.
id="t-843000"&&It was technologists and humanists at that big BiblioTech Conference.
id="t-847000"&&And they gathered together because the technologists wanted to learn what it would be like to think from a humanities perspective.
id="t-854000"&&You have someone from Google talking to someone who does comparative literature.
id="t-858000"&&You're thinking about the relevance of 17th century French theater -- how does that bear upon venture capital?
id="t-858000"&&You're thinking about the relevance of 17th century French theater -- how does that bear upon venture capital?
id="t-858000"&&You're thinking about the relevance of 17th century French theater -- how does that bear upon venture capital?
id="t-866000"&&And when you think in that way, you become more sensitive to the human considerations, which are crucial to making ethical decisions. 

id="t-874000"&&So imagine that right now you went and you found your musician friend.
id="t-878000"&&And you're telling him what we're talking about, about our whole data revolution and all this -- maybe even hum a few bars of our theme music.
id="t-885000"&&♫ Dum ta da da dum dum ta da da dum ♫ Well, your musician friend will stop you and say, "You know, the theme music for your data revolution, that's an opera, that's Wagner.
id="t-896000"&&It's based on Norse legend.
id="t-898000"&&It's Gods and mythical creatures fighting over magical jewelry."
id="t-904000"&&That's interesting.
id="t-907000"&&Now it's also a beautiful opera, and we're moved by that opera.
id="t-913000"&&We're moved because it's about the battle between good and evil, about right and wrong.
id="t-919000"&&And we care about right and wrong.
id="t-921000"&&We care what happens in that opera.
id="t-924000"&&We care what happens in "Apocalypse Now."
id="t-927000"&&And we certainly care what happens with our technologies. 

id="t-931000"&&We have so much power today, it is up to us to figure out what to do, and that's the good news.
id="t-938000"&&We're the ones writing this opera.
id="t-941000"&&This is our movie.
id="t-943000"&&We figure out what will happen with this technology.
id="t-946000"&&We determine how this will all end. 

id="t-949000"&&Thank you. 

id="t-951000"&&(Applause) 

